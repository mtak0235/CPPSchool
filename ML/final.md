# 비선형 분류란?

입력 데이터가 선형적으로 분류되지 않을 때 사용하는 분류 방법

# 비선형 분류 방법 두 가지를 설명하라. 

1. 다항 특성
   * 입력한 데이터에 다항 특성을 추가하여 비선형 데이터를 선형적으로 변환한다.
     즉, 원래 특성이 x인 경우, 새로운 특성으로 x^2, x^3 등을 추가하여 데이터를 변환한다. 
2. 유사 특성
   * 각 샘플과 특정 landmark 간의 유사도를 측정하여 새로운 특성을 만든다. 
3. kernel trick
   * 커널 함수를 사용하여 고차원 공간으로 매핑하는 효과를 주고 계산량도 줄인다. 

# kernel이란?

입력 데이터의 차원을 늘려주는 역할

# 커널 함수 예시 2가지

가우시안 RBF kernel, polynomial kernel

# SVC 작동 방식

1. 데이터 준비
2. 각 클래스의 데이터 포인트를 분리하는 최적의 평면을 찾는다. 
3. 마진에 가장 가까운 데이터 포인트들을 서포트 벡터로 선택한다. 
4. 학습된 모델을 사용해 새로운 데이터 포인트의 클래스를 예측한다. 

# 모델이 과대적합이면 차수를 늘려야 한다. 

ㅗ. 차수를 줄여야 한다. 

# 처음에는 그리드의 폭을 작게 잡는다. 

ㅗ. 크게 잡는다. 

# 유사도 함수란?

각 샘플에 대해 특정 landmark와의 유사도를 측정하는 함수

# 가우시안 RBF 식을 작성하라. 

![image-20240613041930370](./assets/image-20240613041930370.png)

![image-20240613042013289](./assets/image-20240613042013289.png)



# 가우시안 RBF에서 gamma 가 클수록 폭이 넓은 종 모양이다. 

ㅗ. 작을수록

# 가우시안 RBF에서 gamma 가 클수록 샘플의 영향 범위가 좁아진다. 

맞음.

# 가우시안 RBF에서 gamma 가 큰 것은?

1. ![image-20240613043457023](./assets/image-20240613043457023.png)

2. ![image-20240613043508959](./assets/image-20240613043508959.png)

   2결정 경계가 불규칙해지고 샘플을 따라 휘어진다. 

# 다음 중 규제가 작은것은?

![image-20240613043948456](./assets/image-20240613043948456.png)

오른쪽. 규제는 과대적합 담당 일진이다.  C 값은 반대로 따라간다. 

# 유사도 특성을 추가하면 특성의 개수가 샘플의 개수와 같아진다. 

yes

# 유사도 함수의 장단점

장점

* 차원이 커지면서 선형적으로 구분이 될 확률이 높아짐

단점

* 훈련 세트가 매우 클 경우 아주 많은 특성이 생겨버려 ㅈㄴ 연산해야 한다. 

# SVM 회귀란?

마진 위반 발생 정도를 조절하면서 도로 폭을 최대한 넓혀서 도로 위에 가능한 많은 샘플을 포함하는 회귀 방식

#  SVM에게 도로 안 샘플은 마진 위반이다. 

ㅗ. 밖

# SVM 회귀 모델에서 손실 함수는 도로 폭 안에서의 오차에 민감하지 않다. 

맞음.도로폭에 민감하지 않음.

# 도로 폭 안의 오차는 모델의 성능에 영향을 준다. 

ㅗ. 안준다. 마진 폭과 실제 값은 관계가 없다. 그저 허용 오차 범위 일 뿐.

# 손실 함수에 영향을 주지 않는다면 모델의 예측에는 영향이 없다. 

맞음.

# 회귀 모델의 손실 함수

![image-20240613050021067](./assets/image-20240613050021067.png)

# 다음 중 C 값이 큰 것은?

![image-20240613051108920](./assets/image-20240613051108920.png)

왼쪽. 더 샤프하다 == 규제가 약하다 == c가 크다

# 선형 SVM 모델의 결정함수

![image-20240613051311281](./assets/image-20240613051311281.png)

# 결정 함수의 역할

오류가 적거나(soft margin) 없는(hard margin) w를 찾기

# 결정 함수의 기울기가 작을수록 마진 폭도 작아진다. 

ㅗ. 마진 폭은 커진다. 

# 목적 함수 역할

결정함수의 기울이(w) 를 최적화 하기

# decision tree?

* 일련의 분류 규칙을 통해 데이터를 분류하거나 회귀하는 지도 학습 모델.

# 결정 트리 모델의 구조?

* 노드
  * 데이터를 분할하는 기준
* 가지
  * 노드의 결과
  * 다음 노드로 이어짐

# 결정 트리 모델의 동작 원리?

1. root node의 지니 불순도 계산
2. 나머지 속성에 대해 분할 후 자식 노드의 불순도 계산
3. 각 속성에 대한 information gain 계산
4. information gain이 최대가 되는 분기 조건을 찾아 분기
5. 모든 leaf 노드의 불순도가 0이 될 떄 까지 반복

# 실습 

# decision tree 의 주요 장단점

#### 장점

1. **해석 용이성**: 결정트리 모델은 시각적으로 표현이 가능하며, 각 결정 과정이 명확하기 때문에 해석이 쉽습니다.
2. **비선형 관계 표현**: 복잡한 비선형 관계를 잘 포착할 수 있습니다.
3. **전처리 요구사항 낮음**: 데이터의 스케일링이나 정규화 등의 전처리 과정이 필요하지 않습니다.

#### 단점

1. 과적합(Overfitting) 문제
   - 설명: 결정 트리는 모든 데이터를 정확히 분류하려고 하므로, 훈련 데이터에 과적합될 가능성이 높습니다. 이는 특히 깊은 트리에서 두드러지며, 테스트 데이터에 대한 일반화 성능이 저하됩니다.
   - 대응 방법: 가지치기(Pruning), 최대 깊이 제한, 최소 샘플 수 제한 등의 규제 기법 사용.
2. 불안정성(High Variance)
   - 설명: 작은 데이터 변화에도 트리 구조가 크게 변할 수 있습니다. 이는 모델이 매우 민감하게 반응하여 예측 결과가 일관되지 않을 수 있음을 의미합니다.
   - 대응 방법: 배깅(Bagging)과 랜덤 포레스트(Random Forest)와 같은 앙상블 기법을 사용하여 안정성을 향상.
3. 특성 간의 상호작용 고려 어려움
   - 설명: 결정 트리는 한 번에 하나의 특성만을 기준으로 분할하기 때문에, 특성 간의 복잡한 상호작용을 충분히 반영하기 어렵습니다.
   - 대응 방법: 트리 기반의 앙상블 기법 사용.

# Class A : 40개, Class B : 35개, Class C : 25개 인 root node 의 지니 불순도 구하라

# Class A : 40개, Class B : 35개, Class C : 25개 인 root node의 엔트로피 불순도 구하라

# 각 앙상블의 이름과 원리를 비교하라

![image-20240615195038850](./assets/image-20240615195038850.png)



#### 6.2 불순도와 정보 획득
- **불순도 (Impurity)**:
  - 데이터의 혼합 정도를 나타내며, 불순도가 높을수록 데이터가 많이 섞여 있음.
  - **지니 계수(Gini Index)**: 데이터의 통계적 분산 정도.
  - **엔트로피(Entropy)**: 데이터의 불확실성 정도.

- **정보 획득 (Information Gain)**:
  - 분기 이전과 이후의 불순도 차이.
  - 불순도가 더 낮은 쪽으로 선택하여 분기.

#### 6.3 의사결정 나무 구성 단계
1. **루트 노드의 불순도 계산**
2. **각 속성에 대해 분할 후 자식 노드의 불순도 계산**
3. **각 속성에 대한 정보 획득 계산**
4. **정보 획득이 최대가 되는 분기 조건을 찾아 분기**
5. **모든 리프 노드의 불순도가 0이 될 때까지 반복**

#### 6.4 일반화와 가지치기
- **일반화(Generalization)**:
  - 리프 노드가 순도 100%의 한 가지 범주만 가지면 과적합 문제 발생.
  - 가지치기를 통해 과적합을 방지하고 일반화 성능을 높임.

- **가지치기(Pruning)**:
  - **사전 가지치기 (Pre-pruning)**: 트리 생성을 제한.
  - **사후 가지치기 (Post-pruning)**: 완전한 트리 생성 후 가지치기.
  - **비용 함수**:
    \[
    CC(T) = Err(T) + \alpha \times L(T)
    \]
    여기서 \(Err(T)\)는 검증 데이터의 오분류율, \(L(T)\)는 단말 노드 수, \(\alpha\)는 가중치.

#### 6.5 결정 나무 한계
- 샘플 크기가 크면 효율성 및 가독성 저하.
- 과적합 문제로 성능 저하 가능.
- 변수 간 상호작용 파악 어려움.
- 랜덤 포레스트(Random Forest)로 극복 가능.

#### 6.6 사이킷런을 이용한 의사결정 나무
- **학습과 시각화**:
  ```python
  from sklearn.datasets import load_iris
  from sklearn.tree import DecisionTreeClassifier
  iris = load_iris()
  X = iris.data[:, 2:]
  y = iris.target
  tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
  tree_clf.fit(X, y)
  ```

- **결정 나무 시각화**:
  ```python
  from sklearn.tree import export_graphviz
  export_graphviz(tree_clf, out_file="iris_tree.dot")
  ```

#### 6.7 규제 매개변수
- **비매개변수 모델**: 훈련 시작 전에 파라미터 수가 결정되지 않음. 과대적합 위험 높음.
- **매개변수 모델**: 미리 정의된 모델 파라미터 사용. 과대적합 위험 낮음.

- **사이킷런 규제 옵션**:
  - `max_depth`: 트리의 최대 높이.
  - `min_samples_split`: 마디를 분할하기 위한 최소 샘플 수.
  - `min_samples_leaf`: 리프 노드에 포함될 최소 샘플 수.
  - `max_leaf_nodes`: 최대 리프 노드 수.
  - `max_features`: 각 마디에서 분할 평가에 사용할 수 있는 최대 특성 수.

#### 6.8 회귀 트리
- 의사결정 나무 알고리즘을 거의 그대로 사용하여 회귀 문제에 적용.
- **사이킷런 DecisionTreeRegressor** 사용:
  ```python
  from sklearn.tree import DecisionTreeRegressor
  tree_reg = DecisionTreeRegressor(max_depth=2)
  tree_reg.fit(X, y)
  ```

- **회귀 트리 비용 함수**:
  - 평균 제곱 오차(MSE) 최소화 방향으로 학습.

#### 6.9 의사결정 나무의 불안정성
- **훈련 세트의 회전에 민감**: 축에 수직인 분할 사용.
- **훈련 세트의 작은 변화에 민감**: 데이터가 약간만 변경되어도 트리 구조가 크게 변할 수 있음.

이상으로 문서의 6장 내용을 요약하였습니다. 주요 개념과 예제 코드가 포함되어 있어 의사결정 나무의 원리와 활용 방법을 이해하는 데 도움이 될 것입니다.