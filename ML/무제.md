Let's address the questions in your homework #2 document.

### Q1. 결정트리 모델의 구조와 동작 원리에 대해 설명하세요.

### Q2. 결정트리 모델의 주요 장단점 3개씩 작성하세요.

**장점:**
1. **해석 용이성:** 결정트리 모델은 시각적으로 표현이 가능하며, 각 결정 과정이 명확하기 때문에 해석이 쉽습니다.
2. **비선형 관계 표현:** 복잡한 비선형 관계를 잘 포착할 수 있습니다.
3. **전처리 요구사항 낮음:** 데이터의 스케일링이나 정규화 등의 전처리 과정이 필요하지 않습니다.

**단점:**
1. **과적합 위험:** 데이터에 매우 잘 맞출 수 있어 과적합(overfitting)의 위험이 큽니다.
2. **불안정성:** 데이터의 작은 변화에도 모델의 구조가 크게 바뀔 수 있습니다.
3. **계산 복잡성:** 큰 데이터셋에서 결정트리를 생성하는 과정은 계산적으로 비쌉니다.

### Q3. 3개의 클래스가 있는 루트 노드의 지니 불순도를 구하는 과정을 서술하세요.
( Class A : 40개 Class B : 35개 Class C : 25개 )

**지니 불순도(Gini Impurity) 계산 과정:**
1. **각 클래스의 비율 계산:**
   - \( P(A) = \frac{40}{100} = 0.4 \)
   - \( P(B) = \frac{35}{100} = 0.35 \)
   - \( P(C) = \frac{25}{100} = 0.25 \)
2. **지니 불순도 공식 적용:**
   - \( Gini = 1 - \sum P(i)^2 \)
   - \( Gini = 1 - (P(A)^2 + P(B)^2 + P(C)^2) \)
   - \( Gini = 1 - (0.4^2 + 0.35^2 + 0.25^2) \)
   - \( Gini = 1 - (0.16 + 0.1225 + 0.0625) \)
   - \( Gini = 1 - 0.345 = 0.655 \)

### Q4. 3개의 클래스가 있는 루트 노드의 엔트로피 불순도를 구하는 과정을 서술하세요.
( Class A : 40개 Class B : 35개 Class C : 25개 )

**엔트로피(Entropy) 계산 과정:**
1. **각 클래스의 비율 계산:**
   - \( P(A) = \frac{40}{100} = 0.4 \)
   - \( P(B) = \frac{35}{100} = 0.35 \)
   - \( P(C) = \frac{25}{100} = 0.25 \)
2. **엔트로피 공식 적용:**
   - \( Entropy = - \sum P(i) \log_2 P(i) \)
   - \( Entropy = - (P(A) \log_2 P(A) + P(B) \log_2 P(B) + P(C) \log_2 P(C)) \)
   - \( Entropy = - (0.4 \log_2 0.4 + 0.35 \log_2 0.35 + 0.25 \log_2 0.25) \)
   - \( Entropy = - (0.4 \times -1.3219 + 0.35 \times -1.5146 + 0.25 \times -2) \)
   - \( Entropy = - (-0.52876 - 0.53011 - 0.5) \)
   - \( Entropy = 1.55887 \)

### Q5. 다음은 두가지 앙상블 방법을 그림으로 나타낸 것이다. 각각의 방법에 대해 원리를 비교하여 설명하세요.

앙상블 방법에는 **배깅(Bagging)**과 **부스팅(Boosting)**이 있습니다.

**배깅(Bagging):**
- **원리:** 여러 개의 결정트리를 병렬로 학습시켜 각 트리의 예측 결과를 평균내거나 다수결 투표를 통해 최종 예측을 만듭니다.
- **특징:** 각 트리는 독립적으로 학습하며, 전체 데이터셋에서 무작위 복원 추출을 통해 트레이닝 데이터 서브셋을 만듭니다.
- **장점:** 과적합을 줄이고 모델의 안정성을 높입니다.

**부스팅(Boosting):**
- **원리:** 여러 개의 결정트리를 순차적으로 학습시켜 이전 트리의 오류를 보완하는 방식으로 최종 예측을 만듭니다.
- **특징:** 각 트리는 이전 트리의 예측 오류를 중점적으로 학습하며, 가중치를 부여하여 어려운 데이터 포인트에 더 집중합니다.
- **장점:** 예측 성능을 높이는 데 효과적입니다.

이렇게 두 가지 앙상블 방법은 각각의 특성과 장점을 바탕으로 다양한 상황에서 사용됩니다.
